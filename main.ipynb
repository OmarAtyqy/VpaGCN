{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv('data/csv/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the data into two parts according to their VAP (0 or 1) and put them into two variables\n",
    "data_0 = data[data['VAP'] == 0]\n",
    "data_1 = data[data['VAP'] == 1]\n",
    "\n",
    "# print the number of rows of the two variables\n",
    "print(\"Class 0: \", data_0.shape[0])\n",
    "print(\"Class 1: \", data_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values in the TEXT column for each class\n",
    "text_0 = data_0['TEXT'].values\n",
    "text_1 = data_1['TEXT'].values\n",
    "\n",
    "# make a set of all the words in the TEXT column for each class\n",
    "words_0 = set()\n",
    "for text in tqdm(text_0):\n",
    "    for word in text.split():\n",
    "        words_0.add(word)\n",
    "\n",
    "words_1 = set()\n",
    "for text in tqdm(text_1):\n",
    "    for word in text.split():\n",
    "        words_1.add(word)\n",
    "    \n",
    "# make a set of words that are in both classes\n",
    "words_both = set()\n",
    "for word in tqdm(words_0):\n",
    "    if word in words_1:\n",
    "        words_both.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique words in each class\n",
    "print(\"Class 0: \", len(words_0))\n",
    "print(\"Class 1: \", len(words_1))\n",
    "print(\"Both: \", len(words_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "words_to_remove = set()\n",
    "\n",
    "for word in tqdm(words_both):\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "\n",
    "    # count the number of times the word appears in each class\n",
    "    for text in text_0:\n",
    "        if word in text:\n",
    "            count_0 += 1\n",
    "    for text in text_1:\n",
    "        if word in text:\n",
    "            count_1 += 1\n",
    "    \n",
    "    # calculate the probability of the word appearing in each class\n",
    "    prob_0 = count_0 / data_0.shape[0]\n",
    "    prob_1 = count_1 / data_1.shape[0]\n",
    "\n",
    "    # calculate the ratio of the probability of the word appearing in each class\n",
    "    ratio = prob_1 / prob_0\n",
    "\n",
    "    # if the ratio is close to 1 by a certain threshold, remove the word from the set of words that are in both classes\n",
    "    if ratio >= 1 - threshold and ratio <= 1 + threshold:\n",
    "        words_to_remove.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of words to remove\n",
    "print(\"Words to remove: \", len(words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a single regular expression that matches any word in words_to_remove\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_remove) + r')\\b'\n",
    "regex = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    # Replace matched words with a single space\n",
    "    text = regex.sub(' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "print(\"Making a copy of the dataframe...\")\n",
    "data_cpy = data.copy()\n",
    "\n",
    "# remove the words from the TEXT column for each row\n",
    "print(\"Removing the words from the TEXT column for each row...\")\n",
    "for i in tqdm(range(data_cpy.shape[0])):\n",
    "    # remove the words from the TEXT column\n",
    "    text = remove_words(data_cpy.at[i, 'TEXT'])\n",
    "\n",
    "    # if the new text is empty or contains less than 5 words, remove the row\n",
    "    if len(text) == 0 or len(text.split()) < 5:\n",
    "        data_cpy.drop(i, inplace=True)\n",
    "        continue\n",
    "\n",
    "    # update the TEXT column\n",
    "    data_cpy.at[i, 'TEXT'] = text\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "print(\"Saving the dataframe to a csv file...\")\n",
    "data_cpy.to_csv('data/csv/data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix of word embeddings using the cleaned data and the BioWordVec model\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"bin/pubmed.bin\", binary=True)\n",
    "\n",
    "# function to create a matrix of word embeddings\n",
    "def get_word_embeddings(sample):\n",
    "    # initialize a matrix of zeros\n",
    "    embeddings = np.zeros(200)\n",
    "    # get the words in the sample\n",
    "    words = sample.split()\n",
    "    # get the number of words\n",
    "    num_words = len(words)\n",
    "    # loop over the words\n",
    "    for word in words:\n",
    "        # check if the word is in the model's vocabulary\n",
    "        if word in model.key_to_index :\n",
    "            # add the word embedding to the matrix\n",
    "            embeddings += model[word]\n",
    "    # return the matrix divided by the number of words\n",
    "    return np.array(embeddings / num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the TEXT column, for each embedding, create a new column\n",
    "df[[\"embedding_\" + str(i) for i in range(200)]] = df[\"TEXT\"].apply(get_word_embeddings).to_list()\n",
    "\n",
    "# save the dataframe\n",
    "df.to_csv(\"data/csv/data_embedded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_embedded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the class 1 data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# seperate the features and the labels, the features are the word embeddings, each embedding is a column\n",
    "X = df[[col for col in df.columns if \"embedding\" in col]]\n",
    "y = [int(i) for i in df[\"VAP\"].tolist()]\n",
    "\n",
    "# initialize the SMOTE object\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# fit the SMOTE object to the data\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# create a dataframe from the augmented data\n",
    "df_res = pd.DataFrame(X_res, columns=X.columns)\n",
    "df_res[\"VAP\"] = y_res\n",
    "\n",
    "# save the dataframe\n",
    "df_res.to_csv(\"data/csv/data_resampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of samples in each class\n",
    "df_res[\"VAP\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean, chebyshev\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the similarity between the nodes\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# sim matrices\n",
    "sim_dic = {\n",
    "    \"cos_sim\":np.zeros((len(df), len(df))),\n",
    "    \"euc_dist\":np.zeros((len(df), len(df))),\n",
    "    \"cheb_dist\":np.zeros((len(df), len(df)))\n",
    "}\n",
    "\n",
    "# load the embeddings into a matrix\n",
    "print(\"Loading embeddings...\")\n",
    "embeddings = df[[col for col in df.columns if \"embedding\" in col]].to_numpy()\n",
    "\n",
    "# add edges to the graph\n",
    "print(\"Computing similarities...\")\n",
    "progress = tqdm.tqdm(total=len(df))\n",
    "for i in range(len(df)):\n",
    "    for j in range(i+1, len(df)):\n",
    "\n",
    "        # get the embeddings of the two nodes\n",
    "        X = embeddings[i]\n",
    "        Y = embeddings[j]\n",
    "\n",
    "        # compute the similarity between the two nodes using cosine similarity and euclidean distance and chebyshev distance\n",
    "        cos_sim = cosine(X, Y)\n",
    "        euc_dist = euclidean(X, Y)\n",
    "        cheb_dist = chebyshev(X, Y)\n",
    "\n",
    "        # save the similarity matrices\n",
    "        sim_dic[\"cos_sim\"][i, j] = cos_sim\n",
    "        sim_dic[\"euc_dist\"][i, j] = euc_dist\n",
    "        sim_dic[\"cheb_dist\"][i, j] = cheb_dist\n",
    "\n",
    "    # update the progress bar\n",
    "    progress.update(1)\n",
    "\n",
    "# close the progress bar\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity matrices\n",
    "np.save(\"data/sim/cos_sim.npy\", sim_dic[\"cos_sim\"])\n",
    "np.save(\"data/sim/euc_dist.npy\", sim_dic[\"euc_dist\"])\n",
    "np.save(\"data/sim/cheb_dist.npy\", sim_dic[\"cheb_dist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each similarity matrix, plot the distribution of the similarities\n",
    "for sim in sim_dic.keys():\n",
    "\n",
    "    matrix = sim_dic[sim]\n",
    "\n",
    "    # Flatten the matrix and remove diagonal elements\n",
    "    values = matrix[np.triu_indices_from(matrix, k=1)]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(values, kde=True, bins=30)\n",
    "    plt.title('Distribution of ' + sim + ' values')\n",
    "    plt.xlabel('Similarity Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('figures/dist/' + sim + '_distribution.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each similarity matrix, plot the heatmap of the similarities\n",
    "for sim in sim_dic.keys():\n",
    "\n",
    "    matrix = sim_dic[sim]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(matrix, cmap='viridis')\n",
    "    plt.title('Heatmap of ' + sim + ' values')\n",
    "    plt.xlabel('Node ID')\n",
    "    plt.ylabel('Node ID')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('figures/heat/' + sim + '_heatmap.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")\n",
    "distance_matrix = np.load(\"data/sim/euc_dist.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# add nodes to the graph\n",
    "for i in tqdm.tqdm(range(len(df))):\n",
    "    G.add_node(i)\n",
    "\n",
    "# make the VAP column the node labels\n",
    "labels = df[\"VAP\"].to_dict()\n",
    "\n",
    "# add the labels to the graph\n",
    "nx.set_node_attributes(G, labels, \"VAP\")\n",
    "\n",
    "# add edges to the graph\n",
    "for i in tqdm.tqdm(range(len(df))):\n",
    "\n",
    "    # get top 200 most similar nodes\n",
    "    top_200 = np.argsort(distance_matrix[i])[1:201]\n",
    "\n",
    "    # add edges between the node and the top 200 most similar nodes\n",
    "    for j in top_200:\n",
    "        G.add_edge(i, j)\n",
    "\n",
    "# save the graph to a file\n",
    "nx.write_gexf(G, \"data/graphs/graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the number of nodes and edges in the graph\n",
    "print(\"Number of nodes: \", G.number_of_nodes())\n",
    "print(\"Number of edges: \", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graph and df\n",
    "G = nx.read_gexf(\"data/graphs/graph.gexf\")\n",
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding node features to the graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5588/5588 [00:02<00:00, 2410.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# add the node features to the graph\n",
    "print(\"Adding node features to the graph...\")\n",
    "for node in tqdm(G.nodes(data=True)):\n",
    "    \n",
    "    # get the node id\n",
    "    node_id = int(node[0])\n",
    "\n",
    "    # get the node features\n",
    "    node_features = df.iloc[node_id][[col for col in df.columns if \"embedding\" in col]].to_numpy()\n",
    "\n",
    "    # add the node features to the graph\n",
    "    node[1][\"features\"] = torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the adj matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting the adj matrix...\")\n",
    "adj = torch.tensor(nx.to_numpy_matrix(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the graph to a PyG data object\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting the graph to a PyG data object\")\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trainig, validation and test masks\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "test_split = 1 - train_split - val_split\n",
    "\n",
    "# get the number of nodes\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "# Creating indices for splits\n",
    "indices = np.random.permutation(num_nodes)\n",
    "train_size = int(train_split * num_nodes)\n",
    "val_size = int(val_split * num_nodes)\n",
    "\n",
    "# Create boolean masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assigning the masks\n",
    "train_mask[indices[:train_size]] = True\n",
    "val_mask[indices[train_size:train_size + val_size]] = True\n",
    "test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "# add masks to the data\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data onto the device\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 2233642], VAP=[5588], label=[5588], features=[5588, 200], id=[2233642], mode='static', num_nodes=5588, train_mask=[5588], val_mask=[5588], test_mask=[5588])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=16, lr=0.001, weight_decay=5e-4):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(200, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # initialize the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        # initialize the loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # copy the model to the device\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        \n",
    "        # put the model in training mode\n",
    "        self.train()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        progress = tqdm(total=epochs)\n",
    "        for _ in range(epochs):\n",
    "            out = self(data.features[data.train_mask], data.edge_index)\n",
    "            loss += self.criterion(out, data.y[data.train_mask]) / G.number_of_nodes()\n",
    "\n",
    "            # backpropagate the loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # compute the training accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred.eq(data.y[data.train_mask]).sum().item()\n",
    "            train_acc = correct / data.train_mask.sum().item()\n",
    "\n",
    "            # compute the validation accuracy\n",
    "            val_out = self(data.features[data.val_mask], data.edge_index)\n",
    "            pred = val_out.argmax(dim=1)\n",
    "            correct = pred.eq(data.y[data.val_mask]).sum().item()\n",
    "            val_acc = correct / data.val_mask.sum().item()\n",
    "\n",
    "            # update the progress bar\n",
    "            progress.set_description(\"Loss: {:.4f}, Train Acc: {:.4f}, Val Acc: {:.4f}\".format(loss, train_acc, val_acc))\n",
    "            progress.update(1)\n",
    "    \n",
    "    def test(self, data):\n",
    "        self.eval()\n",
    "        out = self(data.features[data.test_mask], data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred.eq(data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\elaty\\OneDrive\\Documents\\School\\S5\\NN\\MedGCN\\main.ipynb Cell 49\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m GCN()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data, \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\elaty\\OneDrive\\Documents\\School\\S5\\NN\\MedGCN\\main.ipynb Cell 49\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m progress \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mepochs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(data\u001b[39m.\u001b[39;49mfeatures[data\u001b[39m.\u001b[39;49mtrain_mask], data\u001b[39m.\u001b[39;49medge_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(out, data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask]) \u001b[39m/\u001b[39m G\u001b[39m.\u001b[39mnumber_of_nodes()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# backpropagate the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\elaty\\OneDrive\\Documents\\School\\S5\\NN\\MedGCN\\main.ipynb Cell 49\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/elaty/OneDrive/Documents/School/S5/NN/MedGCN/main.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m             edge_index \u001b[39m=\u001b[39m cache\n\u001b[1;32m--> 241\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin(x)\n\u001b[0;32m    243\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m    244\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[0;32m    245\u001b[0m                      size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elaty\\AppData\\Local\\Programs\\Python\\Venvs\\Machine Learning\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = GCN()\n",
    "\n",
    "# train the model\n",
    "model.fit(data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # loss += F.cross_entropy(out[data.train_mask], data.y[data.train_mask]) / G.number_of_nodes()\n",
    "    loss += loss_function(out[data.train_mask], data.y[data.train_mask]) / G.number_of_nodes()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm.tqdm(total=epochs)\n",
    "loss = 0\n",
    "for _ in range(epochs):\n",
    "    loss = train(model, optimizer)\n",
    "    progress_bar.set_description(f\"Loss: {loss}\")\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "out = model(data.x, data.edge_index)\n",
    "pred = out.argmax(dim=1)\n",
    "correct += pred.eq(data.y).sum().item()\n",
    "print(\"Accuracy: \", correct / len(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATConv(-1, hidden_channels)  # TODO\n",
    "        self.conv2 = GATConv(hidden_channels, 1)  # TODO\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GAT(hidden_channels=8, heads=8)\n",
    "print(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out[data.train_mask].squeeze(), data.y[data.train_mask].float()) / G.number_of_nodes()\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "\n",
    "progress_bar = tqdm.tqdm(total=epochs)\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    val_acc = test(data.train_mask)\n",
    "    \n",
    "    progress_bar.set_description(f\"Loss: {loss} Val Acc: {val_acc}\")\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# analyse the similarity between the nodes\n",
    "similarity = np.zeros((len(df), len(df)))\n",
    "\n",
    "# add edges to the graph\n",
    "for i in tqdm.tqdm(range(len(df))):\n",
    "\n",
    "    for j in range(len(df)):   \n",
    "\n",
    "        # check if the two nodes are the same\n",
    "        if i != j:\n",
    "\n",
    "            X = df.iloc[i][\"EMBEDDINGS\"]\n",
    "            Y = df.iloc[j][\"EMBEDDINGS\"]\n",
    "\n",
    "            # convert the strings to numpy arrays\n",
    "            X = np.fromstring(X[1:-1], sep=\" \")\n",
    "            Y = np.fromstring(Y[1:-1], sep=\" \")\n",
    "\n",
    "            # compute the similarity between the two nodes\n",
    "            s = np.dot(X, Y)\n",
    "\n",
    "            # add the value to the similarity matrix\n",
    "            similarity[i, j] = s\n",
    "\n",
    "# returns the mean of the similarity matrix\n",
    "print(\"Mean similarity: \", similarity.mean())\n",
    "\n",
    "# returns the maximum value of the similarity matrix\n",
    "print(\"Maximum similarity: \", similarity.max())\n",
    "\n",
    "# returns the minimum value of the similarity matrix\n",
    "print(\"Minimum similarity: \", similarity.min())\n",
    "\n",
    "# returns the standard deviation of the similarity matrix\n",
    "print(\"Standard deviation: \", similarity.std())\n",
    "\n",
    "# returns the variance of the similarity matrix\n",
    "print(\"Variance: \", similarity.var())\n",
    "\n",
    "# returns the median of the similarity matrix\n",
    "print(\"Median: \", np.median(similarity))\n",
    "\n",
    "# returns the 25th percentile of the similarity matrix\n",
    "print(\"25th percentile: \", np.percentile(similarity, 25))\n",
    "\n",
    "# returns the 75th percentile of the similarity matrix\n",
    "print(\"75th percentile: \", np.percentile(similarity, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity matrix\n",
    "np.save(\"similarity.npy\", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
