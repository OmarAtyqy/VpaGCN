{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv('data/csv/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the data into two parts according to their VAP (0 or 1) and put them into two variables\n",
    "data_0 = data[data['VAP'] == 0]\n",
    "data_1 = data[data['VAP'] == 1]\n",
    "\n",
    "# print the number of rows of the two variables\n",
    "print(\"Class 0: \", data_0.shape[0])\n",
    "print(\"Class 1: \", data_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values in the TEXT column for each class\n",
    "text_0 = data_0['TEXT'].values\n",
    "text_1 = data_1['TEXT'].values\n",
    "\n",
    "# make a set of all the words in the TEXT column for each class\n",
    "words_0 = set()\n",
    "for text in tqdm(text_0):\n",
    "    for word in text.split():\n",
    "        words_0.add(word)\n",
    "\n",
    "words_1 = set()\n",
    "for text in tqdm(text_1):\n",
    "    for word in text.split():\n",
    "        words_1.add(word)\n",
    "    \n",
    "# make a set of words that are in both classes\n",
    "words_both = set()\n",
    "for word in tqdm(words_0):\n",
    "    if word in words_1:\n",
    "        words_both.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique words in each class\n",
    "print(\"Class 0: \", len(words_0))\n",
    "print(\"Class 1: \", len(words_1))\n",
    "print(\"Both: \", len(words_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "words_to_remove = set()\n",
    "\n",
    "for word in tqdm(words_both):\n",
    "    count_0 = 0\n",
    "    count_1 = 0\n",
    "\n",
    "    # count the number of times the word appears in each class\n",
    "    for text in text_0:\n",
    "        if word in text:\n",
    "            count_0 += 1\n",
    "    for text in text_1:\n",
    "        if word in text:\n",
    "            count_1 += 1\n",
    "    \n",
    "    # calculate the probability of the word appearing in each class\n",
    "    prob_0 = count_0 / data_0.shape[0]\n",
    "    prob_1 = count_1 / data_1.shape[0]\n",
    "\n",
    "    # calculate the ratio of the probability of the word appearing in each class\n",
    "    ratio = prob_1 / prob_0\n",
    "\n",
    "    # if the ratio is close to 1 by a certain threshold, remove the word from the set of words that are in both classes\n",
    "    if ratio >= 1 - threshold and ratio <= 1 + threshold:\n",
    "        words_to_remove.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of words to remove\n",
    "print(\"Words to remove: \", len(words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a single regular expression that matches any word in words_to_remove\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in words_to_remove) + r')\\b'\n",
    "regex = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    # Replace matched words with a single space\n",
    "    text = regex.sub(' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the dataframe\n",
    "print(\"Making a copy of the dataframe...\")\n",
    "data_cpy = data.copy()\n",
    "\n",
    "# remove the words from the TEXT column for each row\n",
    "print(\"Removing the words from the TEXT column for each row...\")\n",
    "for i in tqdm(range(data_cpy.shape[0])):\n",
    "    # remove the words from the TEXT column\n",
    "    text = remove_words(data_cpy.at[i, 'TEXT'])\n",
    "\n",
    "    # if the new text is empty or contains less than 5 words, remove the row\n",
    "    if len(text) == 0 or len(text.split()) < 5:\n",
    "        data_cpy.drop(i, inplace=True)\n",
    "        continue\n",
    "\n",
    "    # update the TEXT column\n",
    "    data_cpy.at[i, 'TEXT'] = text\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "print(\"Saving the dataframe to a csv file...\")\n",
    "data_cpy.to_csv('data/csv/data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix of word embeddings using the cleaned data and the BioWordVec model\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"bin/pubmed.bin\", binary=True)\n",
    "\n",
    "# function to create a matrix of word embeddings\n",
    "def get_word_embeddings(sample):\n",
    "    # initialize a matrix of zeros\n",
    "    embeddings = np.zeros(200)\n",
    "    # get the words in the sample\n",
    "    words = sample.split()\n",
    "    # get the number of words\n",
    "    num_words = len(words)\n",
    "    # loop over the words\n",
    "    for word in words:\n",
    "        # check if the word is in the model's vocabulary\n",
    "        if word in model.key_to_index :\n",
    "            # add the word embedding to the matrix\n",
    "            embeddings += model[word]\n",
    "    # return the matrix divided by the number of words\n",
    "    return np.array(embeddings / num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the TEXT column, for each embedding, create a new column\n",
    "df[[\"embedding_\" + str(i) for i in range(200)]] = df[\"TEXT\"].apply(get_word_embeddings).to_list()\n",
    "\n",
    "# save the dataframe\n",
    "df.to_csv(\"data/csv/data_embedded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_embedded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the class 1 data using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# seperate the features and the labels, the features are the word embeddings, each embedding is a column\n",
    "X = df[[col for col in df.columns if \"embedding\" in col]]\n",
    "y = [int(i) for i in df[\"VAP\"].tolist()]\n",
    "\n",
    "# initialize the SMOTE object\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# fit the SMOTE object to the data\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "# create a dataframe from the augmented data\n",
    "df_res = pd.DataFrame(X_res, columns=X.columns)\n",
    "df_res[\"VAP\"] = y_res\n",
    "\n",
    "# save the dataframe\n",
    "df_res.to_csv(\"data/csv/data_resampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of samples in each class\n",
    "df_res[\"VAP\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean, chebyshev\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse the similarity between the nodes\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# sim matrices\n",
    "sim_dic = {\n",
    "    \"cos_sim\":np.zeros((len(df), len(df))),\n",
    "    \"euc_dist\":np.zeros((len(df), len(df))),\n",
    "    \"cheb_dist\":np.zeros((len(df), len(df)))\n",
    "}\n",
    "\n",
    "# load the embeddings into a matrix\n",
    "print(\"Loading embeddings...\")\n",
    "embeddings = df[[col for col in df.columns if \"embedding\" in col]].to_numpy()\n",
    "\n",
    "# add edges to the graph\n",
    "print(\"Computing similarities...\")\n",
    "progress = tqdm.tqdm(total=len(df))\n",
    "for i in range(len(df)):\n",
    "    for j in range(i+1, len(df)):\n",
    "\n",
    "        # get the embeddings of the two nodes\n",
    "        X = embeddings[i]\n",
    "        Y = embeddings[j]\n",
    "\n",
    "        # compute the similarity between the two nodes using cosine similarity and euclidean distance and chebyshev distance\n",
    "        cos_sim = cosine(X, Y)\n",
    "        euc_dist = euclidean(X, Y)\n",
    "        cheb_dist = chebyshev(X, Y)\n",
    "\n",
    "        # save the similarity matrices\n",
    "        sim_dic[\"cos_sim\"][i, j] = cos_sim\n",
    "        sim_dic[\"euc_dist\"][i, j] = euc_dist\n",
    "        sim_dic[\"cheb_dist\"][i, j] = cheb_dist\n",
    "\n",
    "    # update the progress bar\n",
    "    progress.update(1)\n",
    "\n",
    "# close the progress bar\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity matrices\n",
    "np.save(\"data/sim/cos_sim.npy\", sim_dic[\"cos_sim\"])\n",
    "np.save(\"data/sim/euc_dist.npy\", sim_dic[\"euc_dist\"])\n",
    "np.save(\"data/sim/cheb_dist.npy\", sim_dic[\"cheb_dist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each similarity matrix, plot the distribution of the similarities\n",
    "for sim in sim_dic.keys():\n",
    "\n",
    "    matrix = sim_dic[sim]\n",
    "\n",
    "    # Flatten the matrix and remove diagonal elements\n",
    "    values = matrix[np.triu_indices_from(matrix, k=1)]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(values, kde=True, bins=30)\n",
    "    plt.title('Distribution of ' + sim + ' values')\n",
    "    plt.xlabel('Similarity Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('figures/dist/' + sim + '_distribution.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each similarity matrix, plot the heatmap of the similarities\n",
    "for sim in sim_dic.keys():\n",
    "\n",
    "    matrix = sim_dic[sim]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(matrix, cmap='viridis')\n",
    "    plt.title('Heatmap of ' + sim + ' values')\n",
    "    plt.xlabel('Node ID')\n",
    "    plt.ylabel('Node ID')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig('figures/heat/' + sim + '_heatmap.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")\n",
    "distance_matrix = np.load(\"data/sim/euc_dist.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# add nodes to the graph\n",
    "print(\"Adding nodes to the graph...\")\n",
    "for i in tqdm(range(len(df))):\n",
    "    G.add_node(i)\n",
    "\n",
    "# make the VAP column the labels of the nodes\n",
    "print(\"Adding labels to the nodes...\")\n",
    "labels = df[\"VAP\"].to_dict()\n",
    "nx.set_node_attributes(G, labels, \"VAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "\n",
    "# add edges to the graph\n",
    "for i in tqdm(range(len(df))):\n",
    "\n",
    "    # get top n most similar nodes\n",
    "    top_n = np.argsort(distance_matrix[i])[1:n+1]\n",
    "\n",
    "    # add edges between the node and the top n most similar nodes\n",
    "    for j in top_n:\n",
    "        if i != j:\n",
    "            G.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph\n",
    "print(\"Saving the graph...\")\n",
    "nx.write_gexf(G, \"data/graphs/graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graph and df\n",
    "G = nx.read_gexf(\"data/graphs/graph.gexf\")\n",
    "df = pd.read_csv(\"data/csv/data_resampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features to the nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5588/5588 [00:02<00:00, 2320.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# add features to the nodes\n",
    "print(\"Adding features to the nodes...\")\n",
    "for node in tqdm(G.nodes(data=True)):\n",
    "    node_id = int(node[0])\n",
    "\n",
    "    node_features = df.iloc[node_id][[col for col in df.columns if \"embedding\" in col]].to_numpy()\n",
    "\n",
    "    # add the node features to the graph\n",
    "    node[1][\"features\"] = torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the adj matrix...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting the adj matrix...\")\n",
    "adj = torch.tensor(nx.to_numpy_matrix(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the graph to a PyG data object\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting the graph to a PyG data object\")\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data onto the device\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 223416], VAP=[5588], label=[5588], features=[5588, 200], id=[223416], mode='static', num_nodes=5588)\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing masks\n",
    "ration = 0.2\n",
    "\n",
    "# get the number of nodes\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "# get the indices of the nodes\n",
    "node_indices = list(range(num_nodes))\n",
    "\n",
    "# shuffle the indices\n",
    "np.random.shuffle(node_indices)\n",
    "\n",
    "# get the number of training nodes\n",
    "num_train = int(ration * num_nodes)\n",
    "\n",
    "# get the training indices\n",
    "train_indices = node_indices[:num_train]\n",
    "\n",
    "# get the testing indices\n",
    "test_indices = node_indices[num_train:]\n",
    "\n",
    "# create the training mask\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "\n",
    "# create the testing mask\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "# load the masks onto the device\n",
    "train_mask = train_mask.to(device)\n",
    "test_mask = test_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, lr=0.001, weight_decay=5e-6):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(200, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # initialize the loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # copy the model to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def fit(self, data, mask, epochs, stop_acc=None):\n",
    "\n",
    "        print(\"Number of training nodes: \", mask.sum())\n",
    "        history = []\n",
    "\n",
    "        progress = tqdm(total=epochs)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # put the model in training mode\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self(data.features, data.edge_index)\n",
    "            loss = self.criterion(out[mask], data.VAP[mask]) / mask.sum().float()\n",
    "\n",
    "            # compute the training accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.VAP[mask]\n",
    "            train_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # update the progress bar\n",
    "            progress.set_description(\"Loss: {}, Train Acc: {}\".format(loss, train_acc))\n",
    "            progress.update(1)\n",
    "\n",
    "            # append the loss and accuracy to the history\n",
    "            history.append([loss, train_acc])\n",
    "\n",
    "            # if the stop_acc is not None and the training accuracy is greater than or equal to the stop_acc, stop training\n",
    "            if stop_acc is not None and train_acc >= stop_acc:\n",
    "                print(\"Training accuracy reached stop_acc, stopping training...\")\n",
    "                break\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def test(self, data, mask):\n",
    "\n",
    "        print(\"Number of testing nodes: \", mask.sum())\n",
    "            \n",
    "        # put the model in eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # get the predictions\n",
    "        out = self(data.features, data.edge_index)\n",
    "\n",
    "        # compute the test accuracy\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data.VAP[mask]\n",
    "        test_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = GCN(\n",
    "    hidden_channels=128,\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-6\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(data,\n",
    "    train_mask,\n",
    "    epochs=5000,\n",
    "    stop_acc=0.97\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loss and accuracy\n",
    "loss = [i[0].cpu().detach().numpy() for i in history]\n",
    "acc = [i[1] for i in history]\n",
    "\n",
    "print(\"Max accuracy: \", max(acc))\n",
    "print(\"Min loss: \", min(loss))\n",
    "\n",
    "# plot and show the loss and accuracy without saving\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "test_acc = model.test(data,\n",
    "                      test_mask\n",
    ")\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, lr=0.001, weight_decay=5e-6):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATConv(-1, hidden_channels, heads=8)\n",
    "        self.conv2 = GATConv(hidden_channels * 8, 2, heads=1)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # initialize the loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # copy the model to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def fit(self, data, mask, epochs, stop_acc=None):\n",
    "\n",
    "        # print(\"Number of training nodes: \", mask.sum())\n",
    "        history = []\n",
    "\n",
    "        progress = tqdm(total=epochs)\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            # put the model in training mode\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self(data.features, data.edge_index)\n",
    "            loss = self.criterion(out[mask], data.VAP[mask]) / mask.sum().float()\n",
    "\n",
    "            # compute the training accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.VAP[mask]\n",
    "            train_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # append the loss and accuracy to the history\n",
    "            history.append([loss, train_acc])\n",
    "\n",
    "            # update the progress bar\n",
    "            progress.set_description(\"Loss: {}, Train Acc: {}\".format(loss, train_acc))\n",
    "            progress.update(1)\n",
    "\n",
    "            # if the stop_acc is not None and the training accuracy is greater than or equal to the stop_acc, stop training\n",
    "            if stop_acc is not None and train_acc >= stop_acc:\n",
    "                print(\"Training accuracy reached stop_acc, stopping training...\")\n",
    "                break\n",
    "\n",
    "        return history\n",
    "\n",
    "    def test(self, data, mask):\n",
    "\n",
    "        # print(\"Number of testing nodes: \", mask.sum())\n",
    "            \n",
    "        # put the model in eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # get the predictions\n",
    "        out = self(data.features, data.edge_index)\n",
    "\n",
    "        # compute the test accuracy\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data.VAP[mask]\n",
    "        test_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model_GAT = GAT(\n",
    "    hidden_channels=128,\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-6\n",
    ")\n",
    "print(model_GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history_GAT = model_GAT.fit(data,\n",
    "    train_mask,\n",
    "    epochs=5000,\n",
    "    stop_acc=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loss and accuracy\n",
    "loss = [i[0].cpu().detach().numpy() for i in history_GAT]\n",
    "acc = [i[1] for i in history_GAT]\n",
    "\n",
    "print(\"Max accuracy: \", max(acc))\n",
    "print(\"Min loss: \", min(loss))\n",
    "\n",
    "# plot and show the loss and accuracy without saving\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acc)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Using Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_channels, lr, weight_decay, dropout, activation_function, optimizer):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(200, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        if optimizer == \"adam\":\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer == \"sgd\":\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            self.optimizer = optim.RMSprop(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer == \"adagrad\":\n",
    "            self.optimizer = optim.Adagrad(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer == \"nadam\":\n",
    "            self.optimizer = optim.NAdam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise Exception(\"Invalid optimizer\")\n",
    "        \n",
    "        # set the activation function\n",
    "        if activation_function == \"relu\":\n",
    "            self.activation_function = F.relu\n",
    "        elif activation_function == \"elu\":\n",
    "            self.activation_function = F.elu\n",
    "        elif activation_function == \"leaky_relu\":\n",
    "            self.activation_function = F.leaky_relu\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            self.activation_function = torch.sigmoid\n",
    "        elif activation_function == \"tanh\":\n",
    "            self.activation_function = torch.tanh\n",
    "        else:\n",
    "            raise Exception(\"Invalid activation function\")\n",
    "        \n",
    "        # set the dropout\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # initialize the loss function\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # keep track of the maximum accuracy\n",
    "        self.max_acc = 0\n",
    "        \n",
    "        # copy the model to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_function(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def fit(self, data, mask, epochs, stop_acc=None):\n",
    "\n",
    "        history = []\n",
    "\n",
    "        progress = tqdm(total=epochs)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # put the model in training mode\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            out = self(data.features, data.edge_index)\n",
    "            loss = self.criterion(out[mask], data.VAP[mask]) / mask.sum().float()\n",
    "\n",
    "            # compute the training accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == data.VAP[mask]\n",
    "            train_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # update the progress bar\n",
    "            progress.set_description(\"Loss: {}, Train Acc: {}\".format(loss, train_acc))\n",
    "            progress.update(1)\n",
    "\n",
    "            # append the loss and accuracy to the history\n",
    "            history.append([loss, train_acc])\n",
    "\n",
    "            # update the max accuracy\n",
    "            if train_acc > self.max_acc:\n",
    "                self.max_acc = train_acc\n",
    "\n",
    "            # if the stop_acc is not None and the training accuracy is greater than or equal to the stop_acc, stop training\n",
    "            if stop_acc is not None and train_acc >= stop_acc:\n",
    "                print(\"Training accuracy reached stop_acc, stopping training...\")\n",
    "                break\n",
    "        \n",
    "        # close the progress bar\n",
    "        progress.close()\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def test(self, data, mask):\n",
    "\n",
    "        # put the model in eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # get the predictions\n",
    "        out = self(data.features, data.edge_index)\n",
    "\n",
    "        # compute the test accuracy\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data.VAP[mask]\n",
    "        test_acc = int(correct.sum()) / int(len(correct))\n",
    "\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuner class for GCN hyperparameters\n",
    "class GCNTuner:\n",
    "\n",
    "    def __init__(self, population_size=50, generations=100, mutation_rate=0.1, epochs=5000):\n",
    "\n",
    "        self.population_size = population_size\n",
    "        self.generations = generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # define the lists of hyperparameters to choose from\n",
    "        self.hidden_channels = [128, 256, 512]\n",
    "        self.lr = np.arange(0.0001, 0.001, 0.0001)\n",
    "        self.weight_decay = np.arange(0.0000001, 0.000001, 0.0000001)\n",
    "        self.dropout = np.arange(0.1, 0.6, 0.1)\n",
    "        self.activation_function = ['relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu']\n",
    "        self.optimizer = ['adam', 'sgd', 'rmsprop', \"adagrad\"]\n",
    "    \n",
    "    def create_individual(self):\n",
    "            \n",
    "        # initialize the hyperparameters\n",
    "        hidden_channels = np.random.choice(self.hidden_channels)\n",
    "        lr = np.random.choice(self.lr)\n",
    "        weight_decay = np.random.choice(self.weight_decay)\n",
    "        dropout = np.random.choice(self.dropout)\n",
    "        activation_function = np.random.choice(self.activation_function)\n",
    "        optimizer = np.random.choice(self.optimizer)\n",
    "\n",
    "        # create the individual\n",
    "        individual = {\n",
    "            \"hidden_channels\": hidden_channels,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"dropout\": dropout,\n",
    "            \"activation_function\": activation_function,\n",
    "            \"optimizer\": optimizer\n",
    "        }\n",
    "\n",
    "        return individual\n",
    "\n",
    "    def compute_fitness(self, individual):\n",
    "        # Initialize the model with the individual's hyperparameters\n",
    "        model = GCN(\n",
    "            hidden_channels=individual[\"hidden_channels\"],\n",
    "            lr=individual[\"lr\"],\n",
    "            weight_decay=individual[\"weight_decay\"],\n",
    "            dropout=individual[\"dropout\"],\n",
    "            activation_function=individual[\"activation_function\"],\n",
    "            optimizer=individual[\"optimizer\"]\n",
    "        )\n",
    "\n",
    "        # Train the model and evaluate it\n",
    "        model.fit(data, train_mask, self.epochs)\n",
    "        \n",
    "        # get the max accuracy and return it as the fitness\n",
    "        return model.max_acc\n",
    "\n",
    "    def mutate(self, individual):\n",
    "        \n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"hidden_channels\"] = np.random.choice(self.hidden_channels)\n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"lr\"] = np.random.choice(self.lr)\n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"weight_decay\"] = np.random.choice(self.weight_decay)\n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"dropout\"] = np.random.choice(self.dropout)\n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"activation_function\"] = np.random.choice(self.activation_function)\n",
    "        if np.random.random() < self.mutation_rate:\n",
    "            individual[\"optimizer\"] = np.random.choice(self.optimizer)\n",
    "        return individual\n",
    "\n",
    "    def crossover(self, parent1, parent2):\n",
    "        child = {}\n",
    "        for key in parent1:\n",
    "            child[key] = parent1[key] if np.random.random() > 0.5 else parent2[key]\n",
    "        return child\n",
    "\n",
    "    def optimize(self):\n",
    "        print(\"Populating...\")\n",
    "        population = [self.create_individual() for _ in range(self.population_size)]\n",
    "\n",
    "        print(\"Evolving...\")\n",
    "        for generation in range(self.generations):\n",
    "\n",
    "            print(f\"Generation {generation + 1} out of {self.generations}\")\n",
    "\n",
    "            # Compute fitness for each individual\n",
    "            fitnesses = [self.compute_fitness(individual) for individual in population]\n",
    "\n",
    "            # Sort the population based on fitness (higher is better)\n",
    "            sorted_population = [x for _, x in sorted(zip(fitnesses, population), key=lambda pair: pair[0], reverse=True)]\n",
    "\n",
    "            # Next generation starts with the top performers (elitism)\n",
    "            next_generation = sorted_population[:2]\n",
    "\n",
    "            # Fill the rest of the next generation with offspring of selected individuals\n",
    "            while len(next_generation) < self.population_size:\n",
    "                parents = np.random.sample(sorted_population[:10], 2)  # Tournament selection\n",
    "                child = self.crossover(parents[0], parents[1])\n",
    "                child = self.mutate(child)\n",
    "                next_generation.append(child)\n",
    "\n",
    "            population = next_generation\n",
    "\n",
    "        # Compute fitness for each individual in the final population\n",
    "        print(\"Computing fitnesses for the final population...\")\n",
    "        fitnesses = [self.compute_fitness(individual) for individual in population]\n",
    "\n",
    "        # return the best individual and the best fitness\n",
    "        best_fitness = max(fitnesses)\n",
    "        best_individual = population[fitnesses.index(best_fitness)]\n",
    "        return best_individual, best_fitness\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        # run the genetic algorithm\n",
    "        print(\"Running the genetic algorithm...\")\n",
    "        best_individual, best_fitness = self.optimize()\n",
    "\n",
    "        # print the best individual and the best fitness\n",
    "        print(\"Best model found with the following hyperparameters:\")\n",
    "        print(\"Best fitness: \", best_fitness)\n",
    "        print(\"Best individual: \", best_individual)\n",
    "\n",
    "        # initialize the model with the best individual's hyperparameters\n",
    "        model = GCN(\n",
    "            hidden_channels=best_individual[\"hidden_channels\"],\n",
    "            lr=best_individual[\"lr\"],\n",
    "            weight_decay=best_individual[\"weight_decay\"],\n",
    "            dropout=best_individual[\"dropout\"],\n",
    "            activation_function=best_individual[\"activation_function\"],\n",
    "            optimizer=best_individual[\"optimizer\"]\n",
    "        )\n",
    "\n",
    "\n",
    "        # train a new model with the best individual's hyperparameters and the stop_acc set to the best fitness\n",
    "        print(\"Training new model with the best individual's hyperparameters...\")\n",
    "        hist = model.fit(data, train_mask, self.epochs, stop_acc=best_fitness)\n",
    "\n",
    "        # get the test accuracy\n",
    "        test_acc = model.test(data, test_mask)\n",
    "\n",
    "        # return the model, the best individual, the best fitness, the test accuracy and the history\n",
    "        return model, best_individual, best_fitness, test_acc, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tuner and run it\n",
    "tuner = GCNTuner(population_size=2, generations=2, mutation_rate=0.1, epochs=100)\n",
    "model, best_individual, best_fitness, test_acc, hist = tuner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
